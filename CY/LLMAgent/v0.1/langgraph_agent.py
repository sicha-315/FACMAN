from langgraph.graph import StateGraph
from langchain_core.runnables import RunnableLambda
from typing import TypedDict
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv
import os

load_dotenv()

# ìƒíƒœ ì •ì˜
class ProcessState(TypedDict):
    runtime: float
    failure_rate: float
    need_maintenance: bool
    
# LLM ì¤€ë¹„
api_key = os.getenv("OPENAI_API_KEY")
llm = ChatOpenAI(model="gpt-4o-mini",
                 api_key=api_key)

# í”„ë¡¬í”„íŠ¸ ì •ì˜
prompt = ChatPromptTemplate.from_template("""
ë‹¹ì‹ ì€ ê³µì • ì ê²€ AIì…ë‹ˆë‹¤.
í•´ë‹¹ ê³µì •ì€ ì œí’ˆ 1ê°œ ìƒì‚°í•˜ëŠ”ë° í‰ê·  10ì´ˆ ì†Œìš”ë©ë‹ˆë‹¤.
ê³ ì¥ ì „ ì ê²€í•  ê²½ìš° í‰ê·  15ì´ˆê°€ ì†Œìš”ë˜ë©°, ê³ ì¥ ì‹œ ìˆ˜ë¦¬í•˜ëŠ” ë° í‰ê·  60ì´ˆê°€ ì†Œìš”ë©ë‹ˆë‹¤.
í˜„ì¬ ê³µì •ì˜ ìƒíƒœëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
- ê°€ë™ ì‹œê°„: {runtime:.2f}ì´ˆ
- ì¶”ì •ëœ ê³ ì¥ í™•ë¥ : {failure_rate:.2f}

ì´ ê³µì •ì„ ì§€ê¸ˆ ì ê²€í•´ì•¼ í•˜ë‚˜ìš”? ê°€ë™ë¥ ì´ ìµœëŒ€ê°€ ë˜ë„ë¡ ê²°ì •í•´ì£¼ì„¸ìš”.
"ì˜ˆ" ë˜ëŠ” "ì•„ë‹ˆì˜¤"ë¡œ ë¨¼ì € ëŒ€ë‹µí•œ í›„, ê·¸ ì´ìœ ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.
""")

# LangGraph ë…¸ë“œ í•¨ìˆ˜: LLMìœ¼ë¡œ íŒë‹¨
def llm_judgment(state: ProcessState) -> ProcessState:
    chain = prompt | llm
    response = chain.invoke(state)
    content = response.content.strip()

    # ì²« ì¤„ì´ "ì˜ˆ" or "ì•„ë‹ˆì˜¤", ê·¸ ì´í›„ëŠ” ì„¤ëª…ì´ë¼ê³  ê°€ì •
    lines = content.split("\n", 1)
    first_line = lines[0].strip()
    reason = lines[1].strip() if len(lines) > 1 else "ì„¤ëª… ì—†ìŒ"

    state["need_maintenance"] = "ì˜ˆ" in first_line
    print("ğŸ§  [LLM íŒë‹¨]")
    print("â†’ íŒë‹¨ ê²°ê³¼:", first_line)
    print("â†’ íŒë‹¨ ê·¼ê±°:", reason)
    return state


# LangGraph ì»´íŒŒì¼ í•¨ìˆ˜
def build_langgraph():
    builder = StateGraph(ProcessState)
    builder.add_node("íŒë‹¨", RunnableLambda(llm_judgment))
    builder.set_entry_point("íŒë‹¨")
    builder.set_finish_point("íŒë‹¨")
    return builder.compile()