import eventlet
eventlet.monkey_patch()

import os
import base64
from io import BytesIO
from flask import Flask, render_template, request, jsonify, send_file
from flask_socketio import SocketIO
from influxdb_client import InfluxDBClient
from openai import OpenAI
from dotenv import load_dotenv
from flask_cors import CORS
from docx import Document
from docx.shared import Inches
from datetime import datetime

from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.memory import MemorySaver
from langchain.agents import Tool
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnableConfig

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# âœ… Flask ì•± ë° SocketIO ì´ˆê¸°í™”
app = Flask(__name__)
CORS(app)
socketio = SocketIO(app, cors_allowed_origins="*", async_mode="eventlet")

# âœ… InfluxDB í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
INFLUX_URL = os.getenv("INFLUX_URL")
INFLUX_TOKEN = os.getenv("INFLUX_TOKEN")
INFLUX_ORG = os.getenv("INFLUX_ORG")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

influx_client = InfluxDBClient(url=INFLUX_URL, token=INFLUX_TOKEN, org=INFLUX_ORG)
openai_client = OpenAI(api_key=OPENAI_API_KEY)

# âœ… ìƒíƒœ emit í•¨ìˆ˜
def emit_status():
    print("[DEBUG] emit_status() ì‹¤í–‰ ì‹œì‘")
    prev_events = {
        "P1-A": None,
        "P1-B": None,
        "P2-A": None,
        "P2-B": None
    }

    while True:
        for bucket, label in [
            ("P1-A_status", "P1-A"),
            ("P1-B_status", "P1-B"),
            ("P2-A_status", "P2-A"),
            ("P2-B_status", "P2-B")
        ]:
            events = get_recent_status(bucket)
            if events:
                latest = events[0]
                if latest != prev_events[label]:
                    print(f"[Influx] {label} ìƒíƒœ ë³€ê²½: {latest}")
                    socketio.emit('status_update', {
                        label: {'event_type': latest}
                    })
                    prev_events[label] = latest
        socketio.sleep(1)

# âœ… ë©”ì¸ í˜ì´ì§€ ë¼ìš°íŒ… ì¶”ê°€
def index():
    return render_template("index.html")
app.route("/")(index)

# âœ… ìœ ìš©ì„± í˜ì´ì§€ ë¼ìš°íŒ… ì¶”ê°€
def usefulness():
    return render_template("usefulness.html")
app.route("/usefulness")(usefulness)

# âœ… ìƒì‚° ì¶”ì´ ë°ì´í„° API ì¶”ê°€
def floor_to_hour(dt):
    return dt.replace(minute=0, second=0, microsecond=0)

@app.route("/get_production_data", methods=["POST"])
def get_production_data():
    try:
        # ğŸ” ì‹¤ì œ ìƒì‚° ì™„ë£Œë§Œ ì¹´ìš´íŠ¸ (status == "finish")
        query = '''
        from(bucket: "process")
        |> range(start: -12h)
        |> filter(fn: (r) => r._measurement == "process_log" and r._field == "status" and r._value == "finish")
        |> group(columns:["line_id"])
        |> aggregateWindow(every: 1h, fn: count, createEmpty: false)
        '''
        result = influx_client.query_api().query(org=INFLUX_ORG, query=query)

        # ê¸°ì¤€ ì‹œê°„ëŒ€ ì •ì˜ (09:00 ~ 18:00)
        time_slots = [f"{h:02}:00" for h in range(9, 19)]
        label_set = set(time_slots)
        data_by_line = {line: {} for line in ["P1-A", "P1-B", "P2-A", "P2-B"]}

        for table in result:
            for record in table.records:
                time_label = record.get_time().strftime("%H:%M")
                line_id = record.values.get("line_id")
                value = int(record.get_value())
                if line_id in data_by_line and time_label in label_set:
                    data_by_line[line_id][time_label] = value

        response = {"labels": time_slots}
        for line_id in data_by_line:
            response[line_id] = [data_by_line[line_id].get(t, 0) for t in time_slots]

        return jsonify(response)

    except Exception as e:
        return jsonify({"error": str(e)}), 500


# âœ… ì±—ë´‡/Langgraph ì„¤ì •
class State(TypedDict):
    messages: Annotated[list, add_messages]

def influxdb_flux_query_tool(process_id: str):
    query_api = influx_client.query_api()

    if not process_id or not isinstance(process_id, str):
        return "ì˜¬ë°”ë¥¸ process_id ë˜ëŠ” line_idë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš” (ì˜ˆ: 'P1' ë˜ëŠ” 'P1-A')"

    if "-" in process_id:
        query = f'''
        from(bucket: "process")
        |> range(start: -1h)
        |> filter(fn: (r) => r._measurement == "process_log")
        |> filter(fn: (r) => r.line_id == "{process_id}")
        '''
    else:
        query = f'''
        from(bucket: "process")
        |> range(start: -1h)
        |> filter(fn: (r) => r._measurement == "process_log")
        |> filter(fn: (r) => r.process_id == "{process_id}")
        '''
    
    try:
        tables = query_api.query(query)
        logs = []
        for table in tables:
            for record in table.records:
                logs.append(f"{record.get_time()}: {record.get_value()}")
        return "\n".join(logs) if logs else f"{process_id}ì— ëŒ€í•œ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤."
    except Exception as e:
        return f"ë¡œê·¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

def create_langgraph_chatbot():
    tools = [Tool(
        name="query_process_logs",
        func=influxdb_flux_query_tool,
        description="Inputì€ 'P1' ë˜ëŠ” 'P1-A'ì™€ ê°™ì€ process_idë‚˜ line_idì…ë‹ˆë‹¤."
    )]

    # âœ… system messageë¥¼ configë¡œ ì§€ì •
    system_prompt = (
        "ë„ˆëŠ” ì œì¡° ê³µì • ë°ì´í„°ë¥¼ ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ì „ë‹¬í•˜ëŠ” ì±—ë´‡ì´ì•¼.\n"
        "â— ë°˜ë“œì‹œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ìµœì¢… ê²°ê³¼ë§Œ ì•Œë ¤ì¤˜. ë°˜ë“œì‹œ í•œ ë¬¸ì¥ìœ¼ë¡œ.\n"
        "ì˜ˆ: 'ìµœê·¼ 1ì‹œê°„ ë™ì•ˆ P2 ì„¤ë¹„ì˜ ê°€ë™ë¥ ì€ ì•½ 75%ì…ë‹ˆë‹¤.'\n"
        "ğŸ”‡ ê³„ì‚° ê³¼ì • ì„¤ëª…, ì„¸ë¶€ ê³„ì‚°, ë¡œê·¸ ëª©ë¡, ë¡œê·¸ ë¶„ì„, ìˆ˜ì‹ ë“±ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆ.\n"
        "â›” 'ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:', 'ê³„ì‚°ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:', 'ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤:' ê°™ì€ í‘œí˜„ì€ ì ˆëŒ€ ê¸ˆì§€ì•¼.\n"
        "â›” ë¬¸ì¥ ëì—ëŠ” ':'ê°€ ì•„ë‹Œ ë°˜ë“œì‹œ ë§ˆì¹¨í‘œë¡œ ëë‚´.\n"
        "ğŸ“Œ '- ì‘ì—… ì‹œì‘ íšŸìˆ˜: 200íšŒ ì´ìƒ'ì²˜ëŸ¼ '-'ë¥¼ ì‚¬ìš©í•  ê²½ìš° ë°˜ë“œì‹œ ì¤„ë°”ê¿ˆí•´ì„œ ë³´ì—¬ì¤˜.\n"
        "ğŸ”š ë§ì¤„ì„ ì—†ì´ ë”± ëŠì–´ì„œ ì •í™•í•˜ê²Œ ë§ˆë¬´ë¦¬í•´ì¤˜."
    )

    # âœ… with_configë¡œ system_message ì „ë‹¬
    llm = ChatOpenAI(model="gpt-4o", temperature=0).with_config(
        configurable={"system_message": system_prompt}
    ).bind_tools(tools)

    def chatbot_node(state: State):
        return {"messages": [llm.invoke(state["messages"])]}

    graph = StateGraph(State)
    graph.add_node("chatbot", chatbot_node)
    graph.add_node("tools", ToolNode(tools=tools))
    graph.add_conditional_edges("chatbot", tools_condition)
    graph.add_edge("tools", "chatbot")
    graph.set_entry_point("chatbot")
    graph.set_finish_point("chatbot")
    return graph.compile(checkpointer=MemorySaver())


# âœ… ìµœê·¼ ì´ë²¤íŠ¸ ìƒíƒœ ì¡°íšŒ í•¨ìˆ˜
def get_recent_status(bucket):
    query = f'''
    from(bucket: "{bucket}")
      |> range(start: -30s)
      |> filter(fn: (r) => r._measurement == "status_log" and r._field == "event_type")
      |> sort(columns: ["_time"], desc: true)
      |> limit(n: 3)
    '''
    result = influx_client.query_api().query(org=INFLUX_ORG, query=query)

    events = []
    for table in result:
        for record in table.records:
            events.append(record.get_value())
    return events if events else None

# âœ… í´ë¼ì´ì–¸íŠ¸ ìµœì´ˆ ì—°ê²° ì‹œ ìƒíƒœ ì „ì†¡
@socketio.on('connect')
def handle_connect():
    print('Client connected')
    for bucket, label in [
        ("P1-A_status", "P1-A"),
        ("P1-B_status", "P1-B"),
        ("P2-A_status", "P2-A"),
        ("P2-B_status", "P2-B")
    ]:
        events = get_recent_status(bucket)
        if events:
            latest = events[0]
            socketio.emit('status_update', {
                label: {'event_type': latest}
            })


# âœ… ì±—ë´‡ API í•¸ë“¤ëŸ¬
@app.route("/chat_langgraph", methods=["POST"])
def chat_langgraph():
    user_message = request.json.get("message", "")
    try:
        graph = create_langgraph_chatbot()
        config = RunnableConfig(recursion_limit=10, configurable={"thread_id": "web-user"})
        response_text = ""
        for event in graph.stream({"messages": [{"role": "user", "content": user_message}]}, config=config):
            for value in event.values():
                if "messages" in value and value["messages"]:
                    response_text = value["messages"][-1].content
        return jsonify({"reply": response_text})
    except Exception as e:
        return jsonify({"reply": f"âŒ LangGraph ì±—ë´‡ ì˜¤ë¥˜: {str(e)}"})


# âœ… ë³´ê³ ì„œ í˜ì´ì§€
@app.route("/report")
def report_page():
    return render_template("report.html")

# âœ… ë³´ê³ ì„œ ìƒì„± API
@app.route("/generate_report", methods=["POST"])
def generate_report():
    data = request.json
    process = data.get("process")
    range_str = data.get("range")
    query = f'''
    from(bucket: "{process}_status")
      |> range(start: -{range_str})
      |> filter(fn: (r) => r._measurement == "status_log")
      |> pivot(rowKey:["_time"], columnKey: ["_field"], valueColumn: "_value")
      |> keep(columns: ["_time", "available", "event_type"])
    '''
    tables = influx_client.query_api().query(query)
    total = 0
    available_sum = 0
    failure_count = 0
    time_labels = []
    available_values = []
    failure_values = []
    for table in tables:
        for record in table.records:
            available = record.values.get("available", 0)
            event_type = record.values.get("event_type", "")
            timestamp = record.values["_time"].strftime("%H:%M")
            total += 1
            available_sum += available
            if event_type == "failure":
                failure_count += 1
            time_labels.append(timestamp)
            available_values.append(round(available, 2))
            failure_values.append(1 if event_type == "failure" else 0)
    avg_avail = round((available_sum / total) * 100, 1) if total else 0
    prompt = f"""
ê³µì •ëª…: {process}
ê¸°ê°„: ìµœê·¼ {range_str}
ê°€ë™ë¥  í‰ê· : {avg_avail}%
ê³ ì¥ íšŸìˆ˜: {failure_count}íšŒ

ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì œì¡° ê³µì • ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì¤˜. ë‹¤ìŒ í•­ëª©ì„ í¬í•¨í•´ì¤˜:
1. ê³µì • ìš”ì•½
2. ì£¼ìš” ì´ìŠˆ
3. ëŒ€ì‘ ì¡°ì¹˜
4. í–¥í›„ ì œì–¸
"""
    try:
        response = openai_client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” ì œì¡°ê³µì • ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ëŠ” AI ë¹„ì„œì•¼."},
                {"role": "user", "content": prompt}
            ]
        )
        return jsonify({
            "report": response.choices[0].message.content,
            "labels": time_labels,
            "available": available_values,
            "failures": failure_values
        })
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        return jsonify({"error": str(e)}), 500

# âœ… ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ API
@app.route("/generate_docx", methods=["POST"])
def generate_docx():
    try:
        data = request.json
        text = data.get("report", "")
        avail_img_b64 = data.get("availabilityImage", "")
        fail_img_b64 = data.get("failureImage", "")
        doc = Document()
        doc.add_heading("\ud83d\udcc4 ìŠ¤ë§ˆíŠ¸ ì œì¡° ë³´ê³ ì„œ", 0)
        doc.add_paragraph(text)
        def add_image(doc, b64_string, title):
            if b64_string and "base64," in b64_string:
                try:
                    doc.add_paragraph(title)
                    image_data = base64.b64decode(b64_string.split(",")[-1])
                    image_stream = BytesIO(image_data)
                    doc.add_picture(image_stream, width=Inches(2.75))
                except Exception as img_err:
                    print(f"ì´ë¯¸ì§€ ë””ì½”ë”© ì˜¤ë¥˜: {img_err}")
        add_image(doc, avail_img_b64, "\u2705 ê°€ë™ë¥  ë³€í™”")
        add_image(doc, fail_img_b64, "\ud83d\udcca ê³ ì¥ ë°œìƒ ë¶„í¬")
        output = BytesIO()
        doc.save(output)
        output.seek(0)
        return send_file(output, as_attachment=True, download_name="ì œì¡°_ë³´ê³ ì„œ.docx")
    except Exception as e:
        print(f"\ud83d\udd1b DOCX ìƒì„± ì˜¤ë¥˜: {e}")
        return jsonify({"error": "íŒŒì¼ ìƒì„± ì‹¤íŒ¨"}), 500

@app.route("/chat", methods=["POST"])
def chat():
    user_message = request.json.get("message", "")
    intent_prompt = f"""
ë„ˆëŠ” AI ì—ì´ì „íŠ¸ì•¼. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³´ê³ , ì•„ë˜ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•´ì¤˜.
- influx: ì œì¡° ê³µì •ì´ë‚˜ ì„¤ë¹„ ìƒíƒœ, ê³ ì¥ ë“± InfluxDBì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì•¼ í•˜ëŠ” ì§ˆë¬¸
- web: ì™¸ë¶€ ë‰´ìŠ¤, ì‹œì„¸, ì¼ë°˜ ì •ë³´ ë“± ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•œ ì§ˆë¬¸
- gpt: ì¼ë°˜ì ì¸ ì§€ì‹ì´ë‚˜ ê°œë… ì„¤ëª…, ì¡ë‹´ ë“±

ì§ˆë¬¸: "{user_message}"
ìœ„ ì§ˆë¬¸ì€ ì–´ë–¤ ìœ í˜•ì´ì•¼? ë°˜ë“œì‹œ influx / web / gpt ì¤‘ í•˜ë‚˜ë§Œ ë§í•´ì¤˜.
"""
    try:
        intent_res = openai_client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ intentë¥¼ ë¶„ì„í•˜ëŠ” íŒë‹¨ ì—ì´ì „íŠ¸ì•¼."},
                {"role": "user", "content": intent_prompt}
            ]
        )
        intent_raw = intent_res.choices[0].message.content.strip().lower()
        intent = intent_raw if intent_raw in ["influx", "web", "gpt"] else "gpt"
        if intent == "influx":
            reply = handle_influx_query(user_message)
        elif intent == "web":
            reply = "\ud83d\udd0d ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ì€ í˜„ì¬ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤."
        else:
            reply = handle_gpt_query(user_message)
        return jsonify({"reply": reply})
    except Exception as e:
        return jsonify({"reply": f"\u26a0\ufe0f ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}), 500

# âœ… GPT ì²˜ë¦¬ í•¨ìˆ˜
def handle_gpt_query(user_message):
    gpt_reply = openai_client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” ì¹œì ˆí•œ ì œì¡° ê³µì • AI ë¹„ì„œì•¼."},
            {"role": "user", "content": user_message}
        ]
    )
    return gpt_reply.choices[0].message.content.strip()

# âœ… Influx ì²˜ë¦¬ í•¨ìˆ˜ (GPT ê¸°ë°˜ ì¿¼ë¦¬ ìƒì„± + ì‹¤í–‰)
def handle_influx_query(user_message):
    query_prompt = f"""
ë„ˆëŠ” InfluxDB ì „ë¬¸ê°€ì•¼. ë‹¤ìŒê³¼ ê°™ì´ ì •í™•í•˜ê²Œ Flux ì¿¼ë¦¬ë¥¼ ìƒì„±í•´ì¤˜.

âš ï¸ ì£¼ì˜ì‚¬í•­:
- ë²„í‚· ì´ë¦„ì€ ë°˜ë“œì‹œ ì•„ë˜ ì¤‘ í•˜ë‚˜ë¡œ ê³ ì •í•´ì•¼ í•´:
  - "P1-A_status"
  - "P1-B_status"
  - "P2-A_status"
  - "P2-B_status"
- "your_bucket" ê°™ì€ í‘œí˜„ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ë©´ ì•ˆ ë¼.
- ì¿¼ë¦¬ëŠ” ì‹¤í–‰ ê°€ëŠ¥í•œ í˜•íƒœì—¬ì•¼ í•˜ê³ , ê²°ê³¼ì— _value ë˜ëŠ” available, event_type í•„ë“œê°€ ìˆì–´ì•¼ í•´.

ì‚¬ìš©ì ì§ˆë¬¸: "{user_message}"
Flux ì¿¼ë¦¬ë§Œ ë°˜í™˜í•´ì¤˜. ì„¤ëª…ì€ í•„ìš” ì—†ì–´.
"""
    try:
        gpt_query_res = openai_client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” InfluxDB ì¿¼ë¦¬ ìƒì„± ì „ë¬¸ê°€ì•¼."},
                {"role": "user", "content": query_prompt}
            ]
        )
        flux_query = gpt_query_res.choices[0].message.content.strip()
        print("âœ… ìƒì„±ëœ ì¿¼ë¦¬:\n", flux_query)
        result_tables = influx_client.query_api().query(flux_query)
        result_rows = []
        for table in result_tables:
            for record in table.records:
                values = record.values
                time_str = str(values.get("_time", "(ì‹œê°„ ì—†ìŒ)"))
                field_str = str(values.get("_field", "(í•„ë“œ ì—†ìŒ)"))
                value_str = str(values.get("_value", values.get("value", "(ê°’ ì—†ìŒ)")))
                result_rows.append(f"{time_str} - {field_str} = {value_str}")
        if not result_rows:
            return "ğŸ“ InfluxDBì—ì„œ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        return "ğŸ“Š InfluxDB ì‘ë‹µ ê²°ê³¼:\n" + "\n".join(result_rows[:10])
    except Exception as e:
        return f"âš ï¸ ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

# âœ… ì„œë²„ ì‹¤í–‰
if __name__ == "__main__":
    socketio.start_background_task(target=emit_status)
    socketio.run(app, host='0.0.0.0', port=5000, debug=False, use_reloader=False, log_output=True)