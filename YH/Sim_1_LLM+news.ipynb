{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ì „ì²´ ì½”ë“œ: LLM + ë‰´ìŠ¤ API + DB ë¼ìš°íŒ… LangGraph ì˜ˆì œ\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from urllib.parse import quote\n",
    "from typing import TypedDict\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# âœ… ìƒíƒœ ì •ì˜\n",
    "class QAState(TypedDict):\n",
    "    question: str\n",
    "    route_label: str\n",
    "    retrieved: str\n",
    "    answer: str\n",
    "\n",
    "# âœ… LLM ì„¤ì •\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3, api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ë¼ìš°íŒ… íŒë‹¨ ë…¸ë“œ\n",
    "def llm_router_update(state: QAState) -> QAState:\n",
    "    prompt = f\"\"\"\n",
    "    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³´ê³  ì•„ë˜ ì¤‘ ì–´ë–¤ ìœ í˜•ì¸ì§€ íŒë‹¨í•˜ì„¸ìš”:\n",
    "\n",
    "    - general: ì¼ë°˜ ì§€ì‹ì´ë‚˜ ê°œë…\n",
    "    - web: ìµœê·¼ ë‰´ìŠ¤, ìµœì‹  ì •ë³´\n",
    "    - db: íšŒì˜ë¡, ë‚´ë¶€ ë°ì´í„° ìš”ì•½\n",
    "\n",
    "    ì§ˆë¬¸: \\\"{state['question']}\\\"\n",
    "\n",
    "    ìœ„ 3ê°€ì§€ ì¤‘ í•˜ë‚˜ì˜ ë‹¨ì–´(general/web/db)ë§Œ ë°˜í™˜í•˜ì„¸ìš”.\n",
    "    \"\"\"\n",
    "    result = llm.invoke(prompt).content.strip().lower()\n",
    "    state[\"route_label\"] = result\n",
    "    print(f\"ğŸ”€ [LLM ë¼ìš°íŒ… íŒë‹¨] â†’ {result}\")\n",
    "    return state\n",
    "\n",
    "# âœ… ì¡°ê±´ ë¶„ê¸° ê¸°ì¤€\n",
    "def get_route(state: QAState) -> str:\n",
    "    return state[\"route_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ì¼ë°˜ ì‘ë‹µ ë…¸ë“œ\n",
    "prompt_general = ChatPromptTemplate.from_template(\"\"\"\n",
    "'{question}'ë¼ëŠ” ì§ˆë¬¸ì— ëŒ€í•´ ì§§ê³  ê°„ê²°í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n",
    "\"\"\")\n",
    "\n",
    "def general_node(state: QAState) -> QAState:\n",
    "    chain = prompt_general | llm\n",
    "    response = chain.invoke({\"question\": state[\"question\"]})\n",
    "    state[\"answer\"] = response.content.strip()\n",
    "    return state\n",
    "\n",
    "# âœ… ë‰´ìŠ¤ ê²€ìƒ‰ API í•¨ìˆ˜\n",
    "def search_naver_news(keyword: str, display: int = 3):\n",
    "    url = f\"https://openapi.naver.com/v1/search/news.json?query={quote(keyword)}&display={display}&sort=date\"\n",
    "    headers = {\n",
    "        \"X-Naver-Client-Id\": os.getenv(\"NAVER_CLIENT_ID\"),\n",
    "        \"X-Naver-Client-Secret\": os.getenv(\"NAVER_CLIENT_SECRET\")\n",
    "    }\n",
    "    res = requests.get(url, headers=headers)\n",
    "    if res.status_code == 200:\n",
    "        items = res.json().get(\"items\", [])\n",
    "        return [f\"- {item['title'].replace('<b>', '').replace('</b>', '')}: {item['link']}\" for item in items]\n",
    "    return [\"ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨\"]\n",
    "\n",
    "# âœ… web ë…¸ë“œ: ë‰´ìŠ¤ API â†’ ìš”ì•½\n",
    "def web_node(state: QAState) -> QAState:\n",
    "    news_list = search_naver_news(state[\"question\"])\n",
    "    state[\"retrieved\"] = \"\\n\".join(news_list)\n",
    "    return state\n",
    "\n",
    "# âœ… db ë…¸ë“œ: (í˜„ì¬ëŠ” Mock í…ìŠ¤íŠ¸)\n",
    "def db_node(state: QAState) -> QAState:\n",
    "    state[\"retrieved\"] = \"[DB ê²€ìƒ‰ ê²°ê³¼] íšŒì˜ ë‚´ìš©ì„ ìš”ì•½í–ˆìŠµë‹ˆë‹¤.\"\n",
    "    return state\n",
    "\n",
    "# âœ… generate ë…¸ë“œ: retrieved ê¸°ë°˜ ìš”ì•½ ìƒì„±\n",
    "prompt_summary = ChatPromptTemplate.from_template(\"\"\"\n",
    "ë‹¤ìŒì€ ë¬¸ì„œ ë˜ëŠ” ë‰´ìŠ¤ ëª©ë¡ì…ë‹ˆë‹¤. í•µì‹¬ì ì¸ ë‚´ìš©ì„ 2~3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”:\n",
    "\n",
    "{retrieved}\n",
    "\"\"\")\n",
    "\n",
    "def generate_node(state: QAState) -> QAState:\n",
    "    if state.get(\"retrieved\"):\n",
    "        chain = prompt_summary | llm\n",
    "        response = chain.invoke({\"retrieved\": state[\"retrieved\"]})\n",
    "        state[\"answer\"] = response.content.strip()\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… LangGraph êµ¬ì„±\n",
    "\n",
    "def build_graph():\n",
    "    builder = StateGraph(QAState)\n",
    "\n",
    "    builder.add_node(\"judge_route\", RunnableLambda(llm_router_update))\n",
    "    builder.add_node(\"general\", RunnableLambda(general_node))\n",
    "    builder.add_node(\"web\", RunnableLambda(web_node))\n",
    "    builder.add_node(\"db\", RunnableLambda(db_node))\n",
    "    builder.add_node(\"generate\", RunnableLambda(generate_node))\n",
    "\n",
    "    builder.set_entry_point(\"judge_route\")\n",
    "    builder.add_conditional_edges(\"judge_route\", get_route, {\n",
    "        \"general\": \"general\",\n",
    "        \"web\": \"web\",\n",
    "        \"db\": \"db\"\n",
    "    })\n",
    "\n",
    "    builder.add_edge(\"general\", \"generate\")\n",
    "    builder.add_edge(\"web\", \"generate\")\n",
    "    builder.add_edge(\"db\", \"generate\")\n",
    "    builder.set_finish_point(\"generate\")\n",
    "\n",
    "    return builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â“ ì§ˆë¬¸: AI Agentë€ ë¬´ì—‡ì¸ê°€ìš”?\n",
      "ğŸ”€ [LLM ë¼ìš°íŒ… íŒë‹¨] â†’ general\n",
      "âœ… ì‘ë‹µ: AI Agentë€ íŠ¹ì • ì‘ì—…ì„ ìˆ˜í–‰í•˜ê±°ë‚˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ì¸ê³µì§€ëŠ¥ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ì´ë“¤ì€ í™˜ê²½ì„ ì¸ì‹í•˜ê³ , ì˜ì‚¬ ê²°ì •ì„ ë‚´ë¦¬ë©°, ì£¼ì–´ì§„ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ í–‰ë™í•©ë‹ˆë‹¤. AI AgentëŠ” ììœ¨ì ìœ¼ë¡œ ì‘ë™í•  ìˆ˜ ìˆìœ¼ë©°, ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë©ë‹ˆë‹¤.\n",
      "\n",
      "â“ ì§ˆë¬¸: ì˜¤ëŠ˜ì˜ ì£¼ìš” ë‰´ìŠ¤ ì•Œë ¤ì¤˜\n",
      "ğŸ”€ [LLM ë¼ìš°íŒ… íŒë‹¨] â†’ web\n",
      "âœ… ì‘ë‹µ: ë‰´ìŠ¤ ê²€ìƒ‰ì— ì‹¤íŒ¨í–ˆë‹¤ëŠ” ë‚´ìš©ìœ¼ë¡œ, íŠ¹ì • ë‰´ìŠ¤ë‚˜ ë¬¸ì„œì— ëŒ€í•œ ì •ë³´ê°€ ì œê³µë˜ì§€ ì•Šê±°ë‚˜ ì ‘ê·¼í•  ìˆ˜ ì—†ëŠ” ìƒí™©ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ë¡œ ì¸í•´ ì›í•˜ëŠ” ì •ë³´ë¥¼ ì°¾ì§€ ëª»í•œ ìƒíƒœì…ë‹ˆë‹¤.\n",
      "\n",
      "â“ ì§ˆë¬¸: ì§€ë‚œ íšŒì˜ ìš”ì•½í•´ì¤˜\n",
      "ğŸ”€ [LLM ë¼ìš°íŒ… íŒë‹¨] â†’ db\n",
      "âœ… ì‘ë‹µ: íšŒì˜ì—ì„œëŠ” ì£¼ìš” ì•ˆê±´ê³¼ ì˜ì‚¬ê²°ì • ì‚¬í•­ì´ ë…¼ì˜ë˜ì—ˆìœ¼ë©°, ì°¸ì„ìë“¤ì€ ê°ìì˜ ì˜ê²¬ì„ ê³µìœ í•˜ê³  í–¥í›„ ê³„íšì— ëŒ€í•´ í•©ì˜í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, íŠ¹ì • ë¬¸ì œì— ëŒ€í•œ í•´ê²° ë°©ì•ˆë„ ì œì‹œë˜ì–´ ì‹¤ì§ˆì ì¸ ì§„í–‰ ë°©í–¥ì´ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# âœ… ì‹¤í–‰ í…ŒìŠ¤íŠ¸\n",
    "if __name__ == \"__main__\":\n",
    "    graph = build_graph()\n",
    "    questions = [\n",
    "        \"AI Agentë€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "        \"ì˜¤ëŠ˜ì˜ ì£¼ìš” ë‰´ìŠ¤ ì•Œë ¤ì¤˜\",\n",
    "        \"ì§€ë‚œ íšŒì˜ ìš”ì•½í•´ì¤˜\"\n",
    "    ]\n",
    "    for q in questions:\n",
    "        print(f\"\\nâ“ ì§ˆë¬¸: {q}\")\n",
    "        result = graph.invoke({\"question\": q, \"route_label\": \"\", \"retrieved\": \"\", \"answer\": \"\"})\n",
    "        print(f\"âœ… ì‘ë‹µ: {result['answer']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
